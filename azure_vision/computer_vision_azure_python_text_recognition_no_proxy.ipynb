{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text recognition with Computer Vision API  <a name=\"RecognizeText\"> </a>\n",
    "* [Detect and extract printed text from an image](#OCR)\n",
    "* [Detect and extract handwritten text from an image](#RecognizeText)\n",
    "\n",
    "Use the [Recognize Text method](https://westcentralus.dev.cognitive.microsoft.com/docs/services/5adf991815e1060e6355ad44/operations/587f2c6a154055056008f200) to asynchronously detect handwritten or printed text in an image and extract recognized characters into a machine-usable character stream.\n",
    "\n",
    "Set `image_path` to point to the image to be recognized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMPORTING THE LIBRARIES\n",
    "import requests\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "import matplotlib.patches as patches\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.patches import Rectangle\n",
    "import time\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "# PROXIES IF YOU ARE RUNNING THE JUPYTER NOTEBOOK FROM YOUR LAPTOP\n",
    "#proxy = \"http://www.com:8080\"\n",
    "#proxys = \"https://www.com:8080\"\n",
    "proxyDict = { \"http\"  : proxy, \"https\"  : proxys }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subscription_key = 'put_your_key_here'\n",
    "assert subscription_key\n",
    "\n",
    "image_path = \"FC_OCR2.PNG\" # CHOOSE YOUR LOCAL IMAGE FOR RECOGNITION HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_base_url = \"https://PYTHONCOMPUTERVISION.cognitiveservices.azure.com/vision/v1.0/\"\n",
    "ocr_url = vision_base_url + \"recognizeText\"\n",
    "print(ocr_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "image_data = open(image_path, \"rb\").read()\n",
    "headers    = {'Ocp-Apim-Subscription-Key': subscription_key, \n",
    "              \"Content-Type\": \"application/octet-stream\" }\n",
    "\n",
    "params   = {'mode' : 'Handwritten'}\n",
    "#data     = {'url': image_url}\n",
    "response = requests.post(ocr_url, headers=headers, params=params, data=image_data) #, proxies=proxyDict)\n",
    "response.raise_for_status()\n",
    "\n",
    "operation_url = response.headers[\"Operation-Location\"]\n",
    "\n",
    "analysis = {}\n",
    "while not \"recognitionResult\" in analysis:\n",
    "    response_final = requests.get(response.headers[\"Operation-Location\"], headers=headers, proxies=proxyDict)\n",
    "    analysis       = response_final.json()\n",
    "    time.sleep(1)\n",
    "\n",
    "polygons = [(line[\"boundingBox\"], line[\"text\"]) for line in analysis[\"recognitionResult\"][\"lines\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the recognized text along with the bounding boxes can be extracted as shown in the following line of code. An important point to note is that the handwritten text recognition API returns bounding boxes as **polygons** instead of **rectangles**. Each polygon is _p_ is defined by its vertices specified using the following convention:\n",
    "\n",
    "<i>p</i> = [<i>x</i><sub>1</sub>, <i>y</i><sub>1</sub>, <i>x</i><sub>2</sub>, <i>y</i><sub>2</sub>, ..., <i>x</i><sub>N</sub>, <i>y</i><sub>N</sub>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING RECTANGLES ON RECOGNIZED TEXT\n",
    "plt.figure(figsize=(25,25))\n",
    "#image  = Image.open(BytesIO(requests.get(image_url).content))\n",
    "image = Image.open(image_path)\n",
    "ax     = plt.imshow(image)\n",
    "for polygon in polygons:\n",
    "    vertices = [(polygon[0][i], polygon[0][i+1]) for i in range(0,len(polygon[0]),2)]\n",
    "    text     = polygon[1]\n",
    "    print(text,vertices)\n",
    "    patch    = Polygon(vertices, closed=True,fill=False, linewidth=2, color='y')\n",
    "    ax.axes.add_patch(patch)\n",
    "    plt.text(vertices[0][0], vertices[0][1], text, fontsize=15, va=\"top\")\n",
    "_ = plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for polygon in polygons:\n",
    "    vertices = [(polygon[0][i], polygon[0][i+1]) for i in range(0,len(polygon[0]),2)]\n",
    "    text     = polygon[1]\n",
    "    print (text,vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "ms_docs_meta": {
   "author": "JuliaNik",
   "description": "Get information and code samples to help you quickly get started using Python and the Computer Vision API in Microsoft Cognitive Services.",
   "manager": "ytkuo",
   "ms.author": "juliakuz",
   "ms.date": "02/02/2018",
   "ms.service": "cognitive-services",
   "ms.technology": "computer-vision",
   "ms.topic": "article",
   "services": "cognitive-services",
   "title": "Computer Vision API Python quick start | Microsoft Docs",
   "titleSuffix": "Computer Vision API Python quick start | Microsoft Cognitive Services"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
